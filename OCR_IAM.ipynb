{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9MzfNPCF9Pi"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import tarfile\n",
        "import os\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from string import printable\n",
        "from functools import reduce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as fn\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install torchinfo -q\n",
        "from torchinfo import summary\n",
        "!pip install torchmetrics -q\n",
        "!pip install Unidecode\n",
        "from torchmetrics.functional.text import char_error_rate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgur_zip_path: str = \"drive/MyDrive/handwritten/imgur/sub_images.zip\"\n",
        "iam_words_tgz_path: str = \"drive/MyDrive/handwritten/words.tgz\"\n",
        "metadata_path: str = \"drive/MyDrive/handwritten/metadata.csv\"\n",
        "\n",
        "imgur_path: str = \"imgur-images\"\n",
        "iam_words_path: str = 'iam-images'\n",
        "\n",
        "def unpack_archive_tgz(path: str, extract_path: str):\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    with tarfile.open(path, 'r') as file:\n",
        "      file.extractall(path=extract_path)\n",
        "\n",
        "def unpack_archive_zip(path: str, extract_path: str):\n",
        "  os.makedirs(extract_path, exist_ok=True)\n",
        "  with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "def get_metadata(imgur_path: str, iam_path: str, metadata_path: str):\n",
        "    metadata = pd.read_csv(metadata_path, index_col=0).reset_index(drop=True)\n",
        "\n",
        "    iam = metadata['database'] == 'iam'\n",
        "    metadata.loc[iam, 'image_path'] = iam_path + '/' + metadata.loc[iam, 'image_path']\n",
        "\n",
        "    imgur = metadata['database'] == 'imgur'\n",
        "    metadata.loc[imgur, 'image_path'] = imgur_path + '/' + metadata.loc[imgur, 'image_path']\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "metadata = get_metadata(\n",
        "    imgur_path='imgur-images/sub_images',\n",
        "    iam_path='iam-images',\n",
        "    metadata_path=metadata_path\n",
        ")\n",
        "\n",
        "unpack_archive_tgz(iam_words_tgz_path, iam_words_path)\n",
        "unpack_archive_zip(imgur_zip_path, imgur_path)"
      ],
      "metadata": {
        "id": "ed28xND1GBPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata"
      ],
      "metadata": {
        "id": "KUUcuj8562VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "def print_shape(w, h, word):\n",
        "  fig, ax = plt.subplots(1,4, figsize=(16,5))\n",
        "  sns.histplot(w, label='width', ax=ax[0])\n",
        "  sns.histplot(h, label='height', ax=ax[1])\n",
        "  sns.histplot(w/h, label='ratio', ax=ax[2])\n",
        "  sns.histplot(word.str.len(), label='label_length', ax=ax[3])\n",
        "  for axe in ax:\n",
        "    axe.legend()\n",
        "\n",
        "print('words metadata')\n",
        "iam_only= (metadata['database'] =='iam')\n",
        "print_shape(metadata.loc[iam_only, 'w'], metadata.loc[iam_only, 'h'],metadata.loc[iam_only, 'word'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7pIKO-YAGBpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterTokenizer:\n",
        "\n",
        "  pad_token: str = \"<PAD>\" # 0\n",
        "  unknown_token: str = \"<UNK>\" # 1\n",
        "  sep_token: str = \"<SEP>\" # 2\n",
        "  void_token: str = \"<VOID>\" # 3 to guarantee 104 tokens in vocab\n",
        "\n",
        "  def __init__(self, max_length: int):\n",
        "    self.special_tokens = [self.pad_token, self.unknown_token, self.sep_token, self.void_token]\n",
        "    self.max_length: int = max_length\n",
        "    self.vocab = build_vocab_from_iterator(printable, specials=self.special_tokens)\n",
        "    self.vocab.set_default_index(self.vocab[self.unknown_token])\n",
        "\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self):\n",
        "    return len(self.vocab)\n",
        "\n",
        "  def get_vocab(self):\n",
        "    return self.vocab.get_stoi()\n",
        "\n",
        "\n",
        "  def __call__(self, sentence: str):\n",
        "    indices = self.vocab.lookup_indices(list(sentence))\n",
        "    sep_indices = []\n",
        "    for indice, next_indice in zip(indices[:-1], indices[1:]):\n",
        "      sep_indices.append(indice)\n",
        "      if indice == next_indice:\n",
        "        sep_indices.append(self.vocab[self.sep_token])\n",
        "\n",
        "    if len(indices) > 0:\n",
        "      sep_indices.append(indices[-1])\n",
        "\n",
        "    if len(sep_indices) >= self.max_length:\n",
        "      return sep_indices[:self.max_length]\n",
        "    return sep_indices + [self.vocab[self.pad_token]]*(self.max_length - len(sep_indices))\n",
        "\n",
        "\n",
        "  def filter_special_tokens(self, token):\n",
        "    return token not in self.special_tokens\n",
        "\n",
        "  def decode(self, ids: list[int] | torch.Tensor, skip_special_tokens: bool = False):\n",
        "    tokens = self.vocab.lookup_tokens(list(ids))\n",
        "    if skip_special_tokens:\n",
        "      tokens = filter(\n",
        "        self.filter_special_tokens,\n",
        "        tokens\n",
        "      )\n",
        "\n",
        "    tokens = reduce(\n",
        "        lambda acc, token: acc + token if acc[-1] != token else acc,\n",
        "        tokens,\n",
        "        \" \"\n",
        "      )\n",
        "    return tokens[1:]"
      ],
      "metadata": {
        "id": "B2b60gqMGEzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class WordsDataset(Dataset):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      tokenizer: CharacterTokenizer,\n",
        "      iam_only: bool = False,\n",
        "      max_length=20,\n",
        "      img_height: int=40,\n",
        "      max_img_width: int = 320,\n",
        "  ):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.img_height = img_height\n",
        "    self.max_length = max_length\n",
        "    self.max_img_width = max_img_width\n",
        "\n",
        "    # prevoir au minimum le double de token de sortie en raison des separators\n",
        "    self.metadata = metadata[\n",
        "            (metadata['word'].str.len() <= self.max_length)\n",
        "        ]\n",
        "\n",
        "    if iam_only:\n",
        "      self.metadata = self.metadata[self.metadata['database'] == 'iam']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.metadata)\n",
        "\n",
        "  def get_resized_image(self, data: pd.Series):\n",
        "    image = Image.open(data[\"image_path\"]).convert('L')\n",
        "    width, height = image.size\n",
        "    aspect_ratio = width / height\n",
        "    img_width = round(self.img_height * aspect_ratio)\n",
        "    image = fn.resize(image, size=[self.img_height, img_width], antialias=None) #type: ignore\n",
        "    image = np.array(image) / 255.\n",
        "\n",
        "    if image.shape[1] >= self.max_img_width:\n",
        "      image = image[:, :self.max_img_width]\n",
        "    else:\n",
        "      image = np.pad(\n",
        "          image,\n",
        "          pad_width=((0,0), (0,self.max_img_width-image.shape[1])),\n",
        "          mode=\"constant\",\n",
        "\n",
        "          constant_values=0\n",
        "      )\n",
        "    return image\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    data: pd.Series = self.metadata.iloc[idx]\n",
        "    label = torch.tensor(self.tokenizer(data['word']))\n",
        "    image = torch.tensor(self.get_resized_image(data),dtype=torch.float32)\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "cbWMMelWGHnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_datasets(\n",
        "    batch_size: int,\n",
        "    max_length: int,\n",
        "    img_height: int,\n",
        "    max_img_width: int,\n",
        "    iam_only: bool = False,\n",
        "    random_state: int = 42,\n",
        "):\n",
        "  tokenizer = CharacterTokenizer(max_length=max_length)\n",
        "  dataset = WordsDataset(img_height=img_height, iam_only=iam_only, max_img_width=max_img_width, max_length=max_length, tokenizer=tokenizer)\n",
        "\n",
        "  train_indices, _test_valid_indices = train_test_split(np.arange(len(dataset)), test_size=0.20, random_state=random_state)\n",
        "  test_indices, valid_indices = train_test_split(_test_valid_indices, test_size=0.50, random_state=random_state)\n",
        "\n",
        "  train_set = Subset(dataset, train_indices)\n",
        "  test_set = Subset(dataset, test_indices)\n",
        "  valid_set = Subset(dataset, valid_indices)\n",
        "\n",
        "  train_loader = DataLoader(train_set,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "  test_loader = DataLoader(test_set,batch_size=batch_size,shuffle=False,drop_last=True)\n",
        "  valid_loader = DataLoader(valid_set,batch_size=batch_size,shuffle=False,drop_last=True)\n",
        "\n",
        "  return {\n",
        "      'train_loader': train_loader,\n",
        "      'test_loader': test_loader,\n",
        "      'valid_loader': valid_loader,\n",
        "      'tokenizer': tokenizer\n",
        "  }\n",
        "\n",
        "datasets = build_datasets(\n",
        "    batch_size=32,\n",
        "    max_length=20,\n",
        "    img_height=40,\n",
        "    iam_only=True,\n",
        "    max_img_width=320,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "9r2SclRCGJx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = datasets['tokenizer']\n",
        "train = datasets['train_loader']\n",
        "test = datasets['test_loader']\n",
        "valid = datasets['valid_loader']"
      ],
      "metadata": {
        "id": "u5h8VLI8GVCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(valid))\n",
        "print(tokenizer.decode(labels[0], skip_special_tokens=True))\n",
        "print(tokenizer.decode(labels[1], skip_special_tokens=True))\n",
        "print(tokenizer.decode(labels[2], skip_special_tokens=True))\n",
        "print(tokenizer.decode(labels[3], skip_special_tokens=True))\n",
        "\n",
        "plt.imshow(images[0])\n",
        "plt.show()\n",
        "plt.imshow(images[1])\n",
        "plt.show()\n",
        "plt.imshow(images[2])\n",
        "plt.show()\n",
        "plt.imshow(images[3])\n"
      ],
      "metadata": {
        "id": "p8Sow1IDGWeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               input_channels: int,\n",
        "               planes: int,\n",
        "               kernel_size: int = 3,\n",
        "               stride: int = 2,\n",
        "               force_downsample: bool = False\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.bn1 = nn.BatchNorm2d(input_channels)\n",
        "    self.conv1 = nn.Conv2d(input_channels, planes, kernel_size=3, stride=stride,padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    if stride != 1 or force_downsample:\n",
        "      self.downsample = nn.Sequential(\n",
        "          nn.Conv2d(input_channels, planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(planes)\n",
        "      )\n",
        "    else:\n",
        "      self.downsample = None\n",
        "\n",
        "\n",
        "  def forward(self, X: torch.Tensor):\n",
        "    x = self.bn1(X)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    skip_connection = self.downsample(X) if self.downsample is not None else X\n",
        "    return x + skip_connection\n",
        "    # return x\n",
        "\n",
        "class TinyOCR(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size: int, seq_size: int):\n",
        "    super().__init__()\n",
        "    self.cnn = nn.Sequential(\n",
        "        ResidualBlock(1,16),\n",
        "        ResidualBlock(16,16, stride=1),\n",
        "        ResidualBlock(16,32),\n",
        "        nn.MaxPool2d((2,1)),\n",
        "        ResidualBlock(32,32, stride=1),\n",
        "        ResidualBlock(32,64, stride=1, force_downsample=True),\n",
        "        ResidualBlock(64,64, stride=1),\n",
        "        ResidualBlock(64,seq_size, stride=1, force_downsample=True),\n",
        "        nn.MaxPool2d((5,1)),\n",
        "    )\n",
        "    self.flatten = nn.Flatten(1,2)\n",
        "    self.rnn = nn.LSTM(input_size=80, hidden_size=128,num_layers=2, bidirectional=True, batch_first=True)\n",
        "    self.linear = nn.Linear(256, vocab_size)\n",
        "    self.softmax = nn.LogSoftmax(-1)\n",
        "\n",
        "  def forward(self, X: torch.Tensor):\n",
        "    x = self.cnn(X)\n",
        "    x = self.flatten(x)\n",
        "    x, hidden_state = self.rnn(x)\n",
        "    x = self.linear(x)\n",
        "    return self.softmax(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "RNCKUH40GcAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Sequential(\n",
        "#         nn.Conv2d(1, 8, kernel_size=3, padding=1, bias=False),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(8),\n",
        "#         nn.MaxPool2d(2),\n",
        "#         nn.Conv2d(8, 16, kernel_size=3, padding=1, bias=False),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(16),\n",
        "#         nn.MaxPool2d((2, 1)),\n",
        "#         nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=False),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(32),\n",
        "#         nn.MaxPool2d((2, 1)),\n",
        "#         nn.Dropout2d(0.2),\n",
        "#         nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(64),\n",
        "#         nn.Dropout2d(0.2),\n",
        "#         nn.Conv2d(64, 64, kernel_size=(4,2), padding='same', bias=False),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(64),\n",
        "#         nn.Dropout2d(0.2),\n",
        "#         nn.MaxPool2d((4,2)),\n",
        "\n",
        "#     )"
      ],
      "metadata": {
        "id": "YOAkBhCFVlja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "class MollyOCR(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size: int):\n",
        "    super().__init__()\n",
        "    resnet = resnet18(weights='IMAGENET1K_V1')\n",
        "    resnet_modules = list(resnet.children())[1:-3]\n",
        "\n",
        "    self.cnn1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
        "        *resnet_modules,\n",
        "        nn.Conv2d(256, 256, kernel_size=(3,6), stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(17,128),\n",
        "    )\n",
        "    self.rnn = nn.LSTM(input_size=768, hidden_size=256,num_layers=2, bidirectional=True, batch_first=True, dropout=0.5)\n",
        "    self.dense = nn.Linear(512, vocab_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "  def forward(self, X: torch.Tensor):\n",
        "    x = self.cnn1(X)\n",
        "    x = x.permute(0, 3, 1,2)\n",
        "    x = x.flatten(-2,-1)\n",
        "    x, hidden = self.rnn(x)\n",
        "    x = self.dense(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "iOnfOYhMa7RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(MollyOCR(104), input_size=(2,1,40,320))\n"
      ],
      "metadata": {
        "id": "fV7RQF8xdyb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(TinyOCR(104, 128), input_size=(2,1,40,320))"
      ],
      "metadata": {
        "id": "WgQbZUPwIKkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4JBidatalxXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch_cer(preds, labels, tokenizer, batch_size):\n",
        "    preds_cpu = preds.cpu().detach().numpy()\n",
        "    labels_cpu = labels.cpu().detach().numpy()\n",
        "\n",
        "    decoded_preds = []\n",
        "    decoded_labels = []\n",
        "    for i in range(batch_size):\n",
        "        decoded_preds.append(\n",
        "            tokenizer.decode(\n",
        "                np.argmax(preds_cpu[i], axis=1),\n",
        "                skip_special_tokens=True,\n",
        "            )\n",
        "        )\n",
        "        decoded_labels.append(\n",
        "            tokenizer.decode(labels_cpu[i], skip_special_tokens=True)\n",
        "        )\n",
        "\n",
        "    return char_error_rate(decoded_preds, decoded_labels).item()"
      ],
      "metadata": {
        "id": "KuZ2XLPvIPMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(\n",
        "    model,\n",
        "    optim,\n",
        "    loss,\n",
        "    batch_size: int,\n",
        "    im_dims: tuple[int, int],\n",
        "    loaders: tuple,\n",
        "    input_length: int,\n",
        "    tokenizer: CharacterTokenizer\n",
        "):\n",
        "  model.train()\n",
        "  total_train_cost = 0\n",
        "  total_valid_cost = 0\n",
        "  total_train_cer = 0\n",
        "  total_valid_cer = 0\n",
        "  for image, label in tqdm(loaders[0], leave=True):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    label = label.to().cuda()\n",
        "    image = image.view(batch_size,1,im_dims[0],im_dims[1]).to().cuda()\n",
        "    preds = model(image)\n",
        "\n",
        "\n",
        "    cost = loss(\n",
        "        preds.transpose(0,1),\n",
        "        label,\n",
        "        torch.full(size=(batch_size,), fill_value=input_length),\n",
        "        torch.count_nonzero(label, dim=1)\n",
        "    )\n",
        "    cost.backward()\n",
        "    optim.step()\n",
        "    total_train_cost += cost.item()\n",
        "    total_train_cer += get_batch_cer(preds, label, tokenizer, batch_size)\n",
        "\n",
        "  train_cer = total_train_cer / len(loaders[0])\n",
        "  train_loss = total_train_cost / len(loaders[0])\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for image, label in loaders[1]:\n",
        "      label = label.to().cuda()\n",
        "      image = image.view(batch_size,1,im_dims[0],im_dims[1]).to().cuda()\n",
        "      preds = model(image)\n",
        "      cost = loss(preds.transpose(0,1), label, torch.full(size=(batch_size,), fill_value=input_length), torch.count_nonzero(label, dim=1)).item()\n",
        "\n",
        "      total_valid_cost += cost\n",
        "      total_valid_cer += get_batch_cer(preds, label, tokenizer, batch_size)\n",
        "\n",
        "    valid_loss = total_valid_cost / len(loaders[1])\n",
        "    valid_cer = total_valid_cer / len(loaders[1])\n",
        "\n",
        "\n",
        "  tqdm.write('\\n')\n",
        "  tqdm.write(f'mean loss train: {train_loss}')\n",
        "  tqdm.write(f'mean loss valid: {valid_loss}')\n",
        "  tqdm.write('\\n')\n",
        "  tqdm.write(f'mean cer train: {train_cer}')\n",
        "  tqdm.write(f'mean cer valid: {valid_cer}')\n",
        "\n",
        "\n",
        "  return train_loss, valid_loss, train_cer, valid_cer"
      ],
      "metadata": {
        "id": "uMrXlUveXYhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    optim,\n",
        "    loss,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    im_dims: tuple[int, int],\n",
        "    loaders: tuple,\n",
        "    input_length: int,\n",
        "    tokenizer: CharacterTokenizer,\n",
        "    scheduler,\n",
        "    patience: int = 4\n",
        "):\n",
        "  total_train_costs = []\n",
        "  total_valid_costs = []\n",
        "  total_train_cer = []\n",
        "  total_valid_cer = []\n",
        "  counter = 0\n",
        "  best_valid_loss = np.inf\n",
        "  for i in range(epochs):\n",
        "    train_loss, valid_loss, train_cer, valid_cer = (\n",
        "        train_epoch(model, optim, loss, batch_size, im_dims, loaders, input_length, tokenizer)\n",
        "    )\n",
        "    total_train_costs.append(train_loss)\n",
        "    total_valid_costs.append(valid_loss)\n",
        "    total_train_cer.append(train_cer)\n",
        "    total_valid_cer.append(valid_cer)\n",
        "\n",
        "\n",
        "    before_lr = optim.param_groups[0][\"lr\"]\n",
        "    scheduler.step(valid_cer)\n",
        "    after_lr = optim.param_groups[0][\"lr\"]\n",
        "    print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (i, before_lr, after_lr))\n",
        "    #  Vérifier la perte de validation\n",
        "    if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      counter = 0\n",
        "    else:\n",
        "      counter += 1\n",
        "      if counter >= patience:\n",
        "          print(f\"Early stopping at epoch {i}. Validation loss did not improve.\")\n",
        "          break\n",
        "\n",
        "  return (\n",
        "      total_train_costs,\n",
        "      total_valid_costs,\n",
        "      total_train_cer,\n",
        "      total_valid_cer,\n",
        "  )"
      ],
      "metadata": {
        "id": "BkudcnDvXesi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "model = MollyOCR(104).to().cuda()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
        "scheduler = ReduceLROnPlateau(optim, verbose=True, patience=5)\n",
        "loss = nn.CTCLoss()\n",
        "epochs = 20\n",
        "# scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "#     optim,\n",
        "#     schedulers=[\n",
        "#         torch.optim.lr_scheduler.LinearLR(optim, start_factor=1/3, end_factor=1.0, total_iters=3),\n",
        "#         torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.5)\n",
        "#     ],\n",
        "#     milestones=[3]\n",
        "# )\n"
      ],
      "metadata": {
        "id": "RCR8TPI0ZK_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(model, input_size=(1,1,40,320))\n",
        "summary(TinyOCR(104, 128), input_size=(1,1,40,320))\n",
        "#"
      ],
      "metadata": {
        "id": "mlnbixGnMCUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_path = \"drive/MyDrive/handwritten/tinyocr_more_output_train.pth\"\n",
        "state = torch.load(state_path)\n",
        "state"
      ],
      "metadata": {
        "id": "l6mJlwJmGIX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(state[\"model\"])\n",
        "optim.load_state_dict(state[\"optim\"])"
      ],
      "metadata": {
        "id": "QGYW099PGhsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for g in optim.param_groups:\n",
        "    g['lr'] = 0.00005"
      ],
      "metadata": {
        "id": "9-g07zmnFRI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    total_train_costs,\n",
        "    total_valid_costs,\n",
        "    total_train_cer,\n",
        "    total_valid_cer,\n",
        ") = train_model(\n",
        "    model, optim, loss, epochs,\n",
        "    batch_size=32, im_dims=(40, 320),\n",
        "    loaders=(train, test),\n",
        "    input_length=80,\n",
        "    tokenizer=tokenizer,\n",
        "    scheduler=scheduler,\n",
        "    patience=10\n",
        "    )"
      ],
      "metadata": {
        "id": "TLrxZjA4Z7-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(819200/320)/40"
      ],
      "metadata": {
        "id": "WACHUHtIzfo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val_costs = []\n",
        "# train_costs = []\n",
        "# train_cer = []\n",
        "# val_cer = []\n",
        "# train_avg_loss = []\n",
        "# val_avg_loss = []\n",
        "val_costs.extend(total_valid_costs)\n",
        "train_costs.extend(total_train_costs)\n",
        "train_cer.extend(total_train_cer)\n",
        "val_cer.extend(total_valid_cer)\n",
        "train_avg_loss.extend(avg_train_costs)\n",
        "val_avg_loss.extend(avg_valid_costs)"
      ],
      "metadata": {
        "id": "BxRXg0_caE1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_result(total_train_costs,\n",
        "    total_valid_costs,\n",
        "    total_train_cer,\n",
        "    total_valid_cer,\n",
        "    avg_train_costs,\n",
        "    avg_valid_costs)"
      ],
      "metadata": {
        "id": "iETpJ6KDoXPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_model_result(total_train_costs,\n",
        "    total_valid_costs,\n",
        "    total_train_cer,\n",
        "    total_valid_cer,\n",
        "    avg_train_costs,\n",
        "    avg_valid_costs):\n",
        "  fig, axes = plt.subplots(3,2, figsize=(10,7))\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  sns.lineplot(x=np.arange(len(total_train_cer[500:])), y=total_train_cer[500:], ax=axes[0], label='train-cer')\n",
        "  sns.lineplot(x=np.arange(len(total_valid_cer)), y=total_valid_cer, ax=axes[1], label='valid-cer')\n",
        "  sns.lineplot(x=np.arange(len(total_train_costs[500:])), y=total_train_costs[500:], ax=axes[2], label='train-loss')\n",
        "  sns.lineplot(x=np.arange(len(total_valid_costs)), y=total_valid_costs, ax=axes[3], label='valid-loss')\n",
        "  sns.lineplot(x=np.arange(len(avg_train_costs)), y=avg_train_costs, ax=axes[4], label='avg-train-loss')\n",
        "  sns.lineplot(x=np.arange(len(avg_valid_costs)), y=avg_valid_costs, ax=axes[4], label='avg-valid-loss')\n",
        "  plt.show()\n",
        "\n",
        "print_model_result(train_costs,\n",
        "    val_costs,\n",
        "    train_cer,\n",
        "    val_cer,\n",
        "    train_avg_loss,\n",
        "    val_avg_loss)"
      ],
      "metadata": {
        "id": "yyCEA3x5aJ35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(total_valid_cer)/len(total_valid_cer)\n"
      ],
      "metadata": {
        "id": "c1a5X6Dl6hzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "results = {\n",
        "    \"train_costs\": train_costs,\n",
        "    \"val_costs\": val_costs,\n",
        "    \"train_cer\": train_cer,\n",
        "    \"val_cer\": val_cer,\n",
        "    \"train_avg_loss\": train_avg_loss,\n",
        "    \"val_avg_loss\": val_avg_loss\n",
        "}\n"
      ],
      "metadata": {
        "id": "2crRBPWWxnKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_object = json.dumps(results, indent=4)\n",
        "\n",
        "# Writing to sample.json\n",
        "with open(\"tiny_ocr_transposed.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ],
      "metadata": {
        "id": "_R99Q9x0x3LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = {\n",
        "    'model': model.state_dict(),\n",
        "    'optim': optim.state_dict(),\n",
        "    'epoch': 20,\n",
        "}\n",
        "torch.save(state, 'tinyocr_transposed_train.pth')"
      ],
      "metadata": {
        "id": "jIoRW1Muc-IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(val_cer[-3000:])/3000"
      ],
      "metadata": {
        "id": "uo8O-IWCEqID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_preds_labels(preds, labels, image, batch_size: int = 200):\n",
        "  decoded_preds = [\n",
        "      tokenizer.decode(\n",
        "          preds[i].argmax(dim=1), skip_special_tokens=True\n",
        "      ) for i in range(batch_size)]\n",
        "\n",
        "  decoded_labels = [\n",
        "      tokenizer.decode(labels[i], skip_special_tokens=True)\n",
        "      for i in range(batch_size)\n",
        "  ]\n",
        "  print('char-error-rate: ', char_error_rate(decoded_preds, decoded_labels).item())\n",
        "  print('\\n\\n')\n",
        "  for pred,label,im in zip(decoded_preds, decoded_labels, image):\n",
        "    print(pred)\n",
        "    plt.imshow(im)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "image, label = next(iter(valid))\n",
        "# image.shape\n",
        "pred = model(image.view(16, 1, 40, 320).to().cuda())\n",
        "compare_preds_labels(pred, label,image.view(16, 40, 320), batch_size=16)"
      ],
      "metadata": {
        "id": "ZRkQXxW8aNDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fp7Vio9UcXpN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}